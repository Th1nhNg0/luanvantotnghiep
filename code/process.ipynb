{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import bs4\n",
        "import requests\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from law_query import LawQuery,Tree\n",
        "from slugify import slugify\n",
        "import gzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # get luat hien hanh\n",
        "# req = requests.get('https://thuvienphapluat.vn/chinh-sach-phap-luat-moi/vn/thoi-su-phap-luat/tu-van-phap-luat/42248/danh-muc-luat-bo-luat-hien-hanh-tai-viet-nam')\n",
        "# soup = BeautifulSoup(req.text, 'html.parser')\n",
        "# luat_hien_hanh = soup.select('.newcontent a')\n",
        "# urls = []\n",
        "# for i in luat_hien_hanh:\n",
        "#     urls.append(i['href'])\n",
        "# urls = urls[1:]\n",
        "# # save to urls.txt\n",
        "# with open('urls.txt', 'w') as f:\n",
        "#     f.write('\\n'.join(urls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "urls = []\n",
        "with open('urls.txt', 'r') as f:\n",
        "    urls = f.read().split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_text(url):\n",
        "    # get metadata\n",
        "    metadata_url='https://thuvienphapluat.vn/AjaxLoadData/LoadLuocDo.aspx?LawID={}&IstraiNghiem=True'\n",
        "    id = url.replace('.aspx','').split('-')[-1]\n",
        "    req=requests.get(metadata_url.format(id),headers={'referer': url})\n",
        "    if req.status_code != 200:\n",
        "        raise Exception('Error when get metadata')\n",
        "    soup = BeautifulSoup(req.text, 'html.parser')\n",
        "    metadata = soup.select_one('#viewingDocument')\n",
        "    metadata = {\n",
        "        'ten_van_ban': metadata.select_one('#viewingDocument > div:nth-child(1)').getText(strip=True), \n",
        "        'so_hieu_van_ban': metadata.select_one('#viewingDocument > div:nth-child(2) > div.ds.fl').getText(strip=True),\n",
        "        'loai_van_ban': metadata.select_one('#viewingDocument > div:nth-child(3) > div.ds.fl').getText(strip=True),\n",
        "        'linhvuc': metadata.select_one('#viewingDocument > div:nth-child(4) > div.ds.fl').getText(strip=True),\n",
        "        'noi_ban_hanh': metadata.select_one('#viewingDocument > div:nth-child(5) > div.ds.fl').getText(strip=True),\n",
        "        'nguoi_ky': metadata.select_one('#viewingDocument > div:nth-child(6) > div.ds.fl').getText(strip=True),\n",
        "        'ngay_ban_hanh': metadata.select_one('#viewingDocument > div:nth-child(7) > div.ds.fl').getText(strip=True),\n",
        "        'ngay_hieu_luc': metadata.select_one('#viewingDocument > div:nth-child(8) > div.ds.fl').getText(strip=True),\n",
        "        'ngay_cong_bao': metadata.select_one('#viewingDocument > div:nth-child(9) > div.ds.fl').getText(strip=True),\n",
        "        'so_cong_bao': metadata.select_one('#viewingDocument > div:nth-child(10) > div.ds.fl').getText(strip=True),\n",
        "        'tinh_trang': metadata.select_one('#viewingDocument > div:nth-child(11) > div.ds.fl').getText(strip=True),\n",
        "    }\n",
        "    req=requests.get(url)\n",
        "    soup = BeautifulSoup(req.text, 'html.parser')\n",
        "    html = soup.find('div', {'class':'content1'})\n",
        "    for element in html(text=lambda text: isinstance(text, bs4.Comment)):\n",
        "        element.extract()\n",
        "    for a in html.find_all('a', href=lambda x: x and x[0] == '#'):\n",
        "        a.extract()\n",
        "    for s in html.find_all(['script','style']):\n",
        "        s.extract()\n",
        "    ps = html.find_all(['p','h1','h2','h3','h4','h5','h6'])\n",
        "    content = ''\n",
        "    for p in ps:\n",
        "        text = p.text\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
        "        content += text + '\\n'\n",
        "    # delete - more than 3\n",
        "    content = re.sub(r'-{3,}', '', content)\n",
        "    content = re.sub(r'\\*{3,}', '', content)\n",
        "    # remove line with empty content\n",
        "    content = re.sub(r'^\\s+$', '', content, flags=re.MULTILINE)\n",
        "    # name = soup.find('h1').text.strip()\n",
        "    matches = re.finditer(r'^(chương [\\d\\w]+.*|phần thứ [\\d\\w]+.*)$', content, re.MULTILINE | re.IGNORECASE)\n",
        "    for i,match in enumerate(matches):\n",
        "        end = match.end()\n",
        "        while end < len(content) and content[end] != '\\n':\n",
        "            end += 1\n",
        "        if end < len(content) and content[end] == '\\n':\n",
        "            content = content[:end] + ':' + content[end+1:]\n",
        "    return metadata,content,html.prettify()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# items = []\n",
        "# for url in urls:\n",
        "#     try:\n",
        "#         metadata,text,html = get_text(url)\n",
        "#         print(metadata['ten_van_ban'])\n",
        "#         name_slug = slugify(metadata['ten_van_ban'])[:50]\n",
        "#         items.append({'metadata':json.dumps(metadata),'path':name_slug})\n",
        "#         folder_path = os.path.join('documents', name_slug)\n",
        "#         os.makedirs(folder_path, exist_ok=True)\n",
        "#         with open(os.path.join(folder_path, 'content.txt'), 'w', encoding='utf-8') as f:\n",
        "#             f.write(text)\n",
        "#         with open(os.path.join(folder_path, 'content.html'), 'w', encoding='utf-8') as f:\n",
        "#             f.write(html)\n",
        "#     except Exception as e:\n",
        "#         print(e)\n",
        "#         print(url)\n",
        "#         continue\n",
        "# df = pd.DataFrame(items)\n",
        "# df.to_csv('documents/data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_content_for_tree(text):\n",
        "    re_footer = r'^(Bộ luật này đã được Quốc hội .*|Luật này được Quốc hội.*|Bộ luật này được Quốc hội .*|Luật này đã được Quốc hội .*)'\n",
        "    matches = re.split(re_footer, text, flags=re.MULTILINE)\n",
        "    text = matches[0]\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CREATE_CONTENT_TREE=False\n",
        "df = pd.read_csv('documents/data.csv')\n",
        "for row in df.itertuples():\n",
        "    print(row.path)\n",
        "    folder_path = os.path.join('documents', row.path)\n",
        "    text = open(os.path.join(folder_path, 'content.txt'), encoding='utf-8').read()\n",
        "    raw_text = text\n",
        "    text = make_content_for_tree(text)\n",
        "    if CREATE_CONTENT_TREE:\n",
        "        with open(os.path.join(folder_path, 'content_tree.txt'), 'w', encoding='utf-8') as f:\n",
        "            f.write(text)\n",
        "    text = open(os.path.join(folder_path, 'content_tree.txt'), encoding='utf-8').read()\n",
        "    tree = Tree(metadata= json.loads(row.metadata),content=text,raw_text=raw_text)\n",
        "    with open(os.path.join(folder_path, 'debug_tree.txt'), 'w', encoding='utf-8') as f:\n",
        "        f.write(str(tree))\n",
        "    with open(os.path.join(folder_path, 'tree.pkl'), 'wb') as f:\n",
        "        pickle.dump(tree, f)\n",
        "    with open(os.path.join(folder_path, 'tree.json'), 'w', encoding='utf-8') as f:\n",
        "        f.write(json.dumps(tree.export(), indent=4, ensure_ascii=False))\n",
        "\n",
        "    with open('./law_query/documents/{}.json.gz'.format(row.path), 'wb') as f:\n",
        "        f.write(gzip.compress(json.dumps(tree.export(), ensure_ascii=False).encode('utf-8')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from law_query_public import Engine,list_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Phần thứ nhất:QUY ĐỊNH CHUNG,\n",
              " Phần thứ hai:QUYỀN SỞ HỮU VÀ QUYỀN KHÁC ĐỐI VỚI TÀI SẢN,\n",
              " Phần thứ ba:NGHĨA VỤ VÀ HỢP ĐỒNG,\n",
              " Phần thứ tư:THỪA KẾ,\n",
              " Phần thứ năm:PHÁP LUẬT ÁP DỤNG ĐỐI VỚI QUAN HỆ DÂN SỰ CÓ YẾU TỐ NƯỚC NGOÀI,\n",
              " Phần thứ sáu:ĐIỀU KHOẢN THI HÀNH]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents = list_documents()\n",
        "query_engine = Engine(documents[0])\n",
        "query_engine.query(node_type='phần')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Chương V:NHÀ NƯỚC CỘNG HÒA XÃ HỘI CHỦ NGHĨA VIỆT NAM, CƠ QUAN NHÀ NƯỚC Ở TRUNG ƯƠNG, Ở ĐỊA PHƯƠNG TRONG QUAN HỆ DÂN SỰ]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_engine.query_by_paths([\n",
        "    {\n",
        "        'node_type': 'phần',\n",
        "        'node_id': 'nhất',\n",
        "    },\n",
        "    {\n",
        "        'node_type': 'chương',\n",
        "        'node_id': 'V',\n",
        "    }\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ten_van_ban: Bộ luật dân sự 2015\n",
              "so_hieu_van_ban: 91/2015/QH13\n",
              "loai_van_ban: Luật\n",
              "linhvuc: Quyền dân sự\n",
              "noi_ban_hanh: Quốc hội\n",
              "nguoi_ky: Nguyễn Sinh Hùng\n",
              "ngay_ban_hanh: 24/11/2015\n",
              "ngay_hieu_luc: 01/01/2017\n",
              "ngay_cong_bao: 28/12/2015\n",
              "so_cong_bao: Từ số 1243 đến số 1244\n",
              "tinh_trang: Còn hiệu lực"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_engine"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "research",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

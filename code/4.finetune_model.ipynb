{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import lawquery\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize,word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import string\n",
    "\n",
    "def format_text(text,word_segmentation=False):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.strip()\n",
    "    text = text_normalize(text)\n",
    "    if word_segmentation:\n",
    "        text = word_tokenize(text, format=\"text\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of engines: 15\n",
      "Number of documents: 740\n"
     ]
    }
   ],
   "source": [
    "engines = {}\n",
    "law_df = pd.read_csv('./documents/data.csv')\n",
    "for i in range(len(law_df)):\n",
    "    path = law_df['path'][i]\n",
    "    so_hieu_van_ban = law_df['so_hieu_van_ban'][i]\n",
    "    if so_hieu_van_ban not in engines:\n",
    "        engines[so_hieu_van_ban] = lawquery.Engine(\n",
    "            os.path.join('documents',path, 'tree.json.gz'),\n",
    "        )\n",
    "print(\"Number of engines:\",len(engines))\n",
    "\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "\n",
    "# take all\n",
    "for engine in engines:\n",
    "    results = engines[engine].query(node_type='điều')\n",
    "    for result in results:\n",
    "        documents.append(result.name+'\\n'+result.content)\n",
    "        metadatas.append({'law_id': engine, 'node_type': result.node_type, 'node_id': result.node_id})\n",
    "        ids.append(result.id)\n",
    "\n",
    "qa_df = pd.read_json('./answers_filtered.jsonl', lines=True, orient='records')\n",
    "print(\"Number of documents:\",len(documents))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training triplet using TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdidf_docs= [format_text(doc,True) for doc in documents]\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(tdidf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructor_for_q = 'Represent the legal question for retrieving evidence documents:'\n",
    "instructor_for_r = 'Represent the legal document for retrieval:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4205/4205 [00:24<00:00, 173.63it/s]\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "to_run = len(qa_df)\n",
    "for i in tqdm(range(to_run)):\n",
    "    row = qa_df.iloc[i]\n",
    "    data = {}\n",
    "    query = row['cauhoi']\n",
    "    pos = []\n",
    "    for item in row['new_answers']:\n",
    "        idx = metadatas.index(item)\n",
    "        pos.append(documents[idx])\n",
    "    query = format_text(query,True) \n",
    "    query_vector = tfidf.transform([query])\n",
    "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    related_docs_indices = cosine_similarities.argsort()[:-100-1:-1]\n",
    "    results = []\n",
    "    for j in related_docs_indices:\n",
    "        results.append(documents[j])\n",
    "    neg = []\n",
    "    for j in range(len(results)):\n",
    "        if results[j] not in pos:\n",
    "            neg.append(results[j])\n",
    "        if len(neg) == len(pos):\n",
    "            break\n",
    "\n",
    "    for j in range(len(pos)):\n",
    "        data = {}\n",
    "        data['pos'] = [instructor_for_r,pos[j]]\n",
    "        data['neg'] = [instructor_for_r,neg[j]]\n",
    "        data['query'] = [instructor_for_q,query]\n",
    "        data['task_name'] = 'tracuuluat'\n",
    "        datasets.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6446"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gzip\n",
    "\n",
    "# with gzip.open('medi-data.json.gz', 'wt', encoding='utf-8') as fout:\n",
    "#     fout.write(json.dumps(datasets, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training triplet using SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ngoph\\.conda\\envs\\me\\lib\\site-packages\\InstructorEmbedding\\instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "from InstructorEmbedding import INSTRUCTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client(Settings(chroma_api_impl=\"rest\",\n",
    "                                    chroma_server_host=\"localhost\",\n",
    "                                    chroma_server_http_port=\"8000\",\n",
    "                                    chroma_server_ssl_enabled=False\n",
    "                                    ))\n",
    "collection = chroma_client.get_or_create_collection(name=\"law_documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "model = INSTRUCTOR('C:/Users/ngoph/Desktop/luanvan/model')\n",
    "instructor_for_q = 'Represent the legal question for retrieving evidence documents:'\n",
    "instructor_for_r = 'Represent the legal document for retrieval:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Cấp lại sổ BHXH do mất, hỏng, gộp sổ BHXH\\n1.1. Thành phần hồ sơ:\\na) Cấp lại sổ BHXH do mất, hỏng: Tờ khai tham gia, điều chỉnh thông tin BHXH, BHYT (Mẫu TK1-TS). b) Gộp sổ BHXH:\\n- Tờ khai tham gia, điều chỉnh thông tin BHXH, BHYT (Mẫu TK1-TS).\\n- Các sổ BHXH đề nghị gộp (nếu có);\\n1.2. Số lượng hồ sơ: 01 bộ.\\n2. Cấp lại sổ BHXH do thay đổi họ, tên, chữ đệm; ngày, tháng, năm sinh; giới tính, dân tộc; quốc tịch; điều chỉnh nội dung trên sổ BHXH\\n2.1. Thành phần hồ sơ\\na) Người tham gia\\n- Tờ khai tham gia, điều chỉnh thông tin BHXH, BHYT (Mẫu TK1-TS).\\n- Hồ sơ kèm theo (Mục 3,4 Phụ lục 01).\\nb) Đơn vị: Bảng kê thông tin (Mẫu D01-TS).\\n2.2. Số lượng hồ sơ: 01 bộ.\\n3. Ghi xác nhận thời gian đóng BHXH cho người tham gia được cộng nối thời gian nhưng không phải đóng BHXH và điều chỉnh làm nghề hoặc công việc nặng nhọc, độc hại, nguy hiểm hoặc đặc biệt nặng nhọc, độc hại, nguy hiểm trước năm 1995\\n3.1. Thành phần hồ sơ\\na) Tờ khai tham gia, điều chỉnh thông tin BHXH, BHYT (Mẫu TK1-TS).\\nb) Hồ sơ kèm theo (Mục 1, 2 Phụ lục 01).\\n3.2. Số lượng hồ sơ: 01 bộ.\\n4. Cấp lại, đổi thẻ BHYT\\n4.1. Thành phần hồ sơ\\na) Người tham gia\\n- Tờ khai tham gia, điều chỉnh thông tin BHXH, BHYT (Mẫu TK1-TS).\\n-Trường hợp người lao động được hưởng quyền lợi BHYT cao hơn: bổ sung Giấy tờ chứng minh (nếu có) theo Phụ lục 03.\\nb) Đơn vị: Bảng kê thông tin (Mẫu D01-TS).\\n4.2. Số lượng hồ sơ: 01 bộ.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engines['2089/VBHN-BHXH'].query(node_type='điều',node_id='27')[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 2927/4205 [01:50<00:48, 26.55it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     idx \u001b[39m=\u001b[39m metadatas\u001b[39m.\u001b[39mindex(item)\n\u001b[0;32m     10\u001b[0m     pos\u001b[39m.\u001b[39mappend(documents[idx])\n\u001b[1;32m---> 12\u001b[0m query_embeddings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode([[instructor_for_q,query]],show_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m     13\u001b[0m results \u001b[39m=\u001b[39m collection\u001b[39m.\u001b[39mquery(query_embeddings\u001b[39m=\u001b[39mquery_embeddings, n_results\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,include\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mmetadatas\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     14\u001b[0m results \u001b[39m=\u001b[39m results[\u001b[39m'\u001b[39m\u001b[39mmetadatas\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ngoph\\.conda\\envs\\me\\lib\\site-packages\\InstructorEmbedding\\instructor.py:535\u001b[0m, in \u001b[0;36mINSTRUCTOR.encode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[39mfor\u001b[39;00m start_index \u001b[39min\u001b[39;00m trange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(sentences), batch_size, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatches\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n\u001b[0;32m    534\u001b[0m     sentences_batch \u001b[39m=\u001b[39m sentences_sorted[start_index:start_index\u001b[39m+\u001b[39mbatch_size]\n\u001b[1;32m--> 535\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenize(sentences_batch)\n\u001b[0;32m    536\u001b[0m     features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[0;32m    538\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\ngoph\\.conda\\envs\\me\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:319\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, texts: Union[List[\u001b[39mstr\u001b[39m], List[Dict], List[Tuple[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]]):\n\u001b[0;32m    316\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39m    Tokenizes the texts\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_first_module()\u001b[39m.\u001b[39;49mtokenize(texts)\n",
      "File \u001b[1;32mc:\\Users\\ngoph\\.conda\\envs\\me\\lib\\site-packages\\InstructorEmbedding\\instructor.py:354\u001b[0m, in \u001b[0;36mINSTRUCTOR_Transformer.tokenize\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(contexts[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\u001b[39mstr\u001b[39m)\n\u001b[0;32m    353\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(concatenated_input_texts[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\u001b[39mstr\u001b[39m)\n\u001b[1;32m--> 354\u001b[0m tokenized \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenize(concatenated_input_texts)\n\u001b[0;32m    355\u001b[0m context_tok \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenize(contexts)\n\u001b[0;32m    356\u001b[0m tokenized[\u001b[39m'\u001b[39m\u001b[39mcontext_masks\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(context_tok[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m],dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ngoph\\.conda\\envs\\me\\lib\\site-packages\\InstructorEmbedding\\instructor.py:322\u001b[0m, in \u001b[0;36mINSTRUCTOR_Transformer.tokenize\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_lower_case:\n\u001b[0;32m    320\u001b[0m         to_tokenize \u001b[39m=\u001b[39m [[s\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m col] \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m to_tokenize]\n\u001b[1;32m--> 322\u001b[0m     tokenized \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer(\u001b[39m*\u001b[39;49mto_tokenize, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, truncation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlongest_first\u001b[39;49m\u001b[39m'\u001b[39;49m, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_seq_length)\n\u001b[0;32m    324\u001b[0m \u001b[39m# elif isinstance(texts[0], dict):\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[39m#     to_tokenize = []\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[39m#     output['text_keys'] = []\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[39m#         output['text_keys'].append(text_key)\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[39m#     to_tokenize = [to_tokenize]\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(texts[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ngoph\\.conda\\envs\\me\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2561\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2559\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2560\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2561\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_one(text\u001b[39m=\u001b[39mtext, text_pair\u001b[39m=\u001b[39mtext_pair, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_kwargs)\n\u001b[0;32m   2562\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2563\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\ngoph\\.conda\\envs\\me\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2647\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2642\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2643\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2644\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2645\u001b[0m         )\n\u001b[0;32m   2646\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[1;32m-> 2647\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2648\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2649\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2650\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2651\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   2652\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2653\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2654\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2655\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2656\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2657\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2658\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2659\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2660\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2661\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2662\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2663\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2664\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2665\u001b[0m     )\n\u001b[0;32m   2666\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2667\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2668\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2669\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2685\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2686\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ngoph\\.conda\\envs\\me\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2838\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2828\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2829\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2830\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2831\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2835\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2836\u001b[0m )\n\u001b[1;32m-> 2838\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   2839\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2840\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2841\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2842\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2843\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2844\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2845\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2846\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2847\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2848\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2849\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2850\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2851\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2852\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2853\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2854\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2855\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2856\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ngoph\\.conda\\envs\\me\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:425\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[0;32m    418\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m    419\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    423\u001b[0m )\n\u001b[1;32m--> 425\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode_batch(\n\u001b[0;32m    426\u001b[0m     batch_text_or_text_pairs,\n\u001b[0;32m    427\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m    428\u001b[0m     is_pretokenized\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[0;32m    429\u001b[0m )\n\u001b[0;32m    431\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[0;32m    437\u001b[0m tokens_and_encodings \u001b[39m=\u001b[39m [\n\u001b[0;32m    438\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_encoding(\n\u001b[0;32m    439\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m encodings\n\u001b[0;32m    449\u001b[0m ]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "to_run = len(qa_df)\n",
    "for i in tqdm(range(to_run)):\n",
    "    row = qa_df.iloc[i]\n",
    "    data = {}\n",
    "    query = row['cauhoi']\n",
    "    pos = []\n",
    "    for item in row['new_answers']:\n",
    "        idx = metadatas.index(item)\n",
    "        pos.append(documents[idx])\n",
    "    \n",
    "    query_embeddings = model.encode([[instructor_for_q,query]],show_progress_bar=False).tolist()\n",
    "    results = collection.query(query_embeddings=query_embeddings, n_results=100,include=[\"metadatas\"])\n",
    "    results = results['metadatas'][0]\n",
    "    neg = []\n",
    "    for result in results:\n",
    "        doc = engines[result['law_id']].query(node_type=result['node_type'],node_id=result['node_id'])[0]\n",
    "        doc = doc.name+'\\n'+doc.content\n",
    "        if doc not in pos:\n",
    "            neg.append(doc)\n",
    "        if len(neg) == len(pos):\n",
    "            break\n",
    "\n",
    "    for j in range(len(pos)):\n",
    "        data = {}\n",
    "        data['pos'] = [instructor_for_r,pos[j]]\n",
    "        data['neg'] = [instructor_for_r,neg[j]]\n",
    "        data['query'] = [instructor_for_q,query]\n",
    "        data['task_name'] = 'tracuuluat'\n",
    "        datasets.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('finetune-data-s2.json.gz', 'wt', encoding='utf-8') as fout:\n",
    "    fout.write(json.dumps(datasets, ensure_ascii=False, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

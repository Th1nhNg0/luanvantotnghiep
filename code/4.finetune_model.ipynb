{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import lawquery\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import text_normalize,word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import string\n",
    "\n",
    "def format_text(text,word_segmentation=False):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.strip()\n",
    "    text = text_normalize(text)\n",
    "    if word_segmentation:\n",
    "        text = word_tokenize(text, format=\"text\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of engines: 15\n",
      "Number of documents: 740\n"
     ]
    }
   ],
   "source": [
    "engines = {}\n",
    "law_df = pd.read_csv('./documents/data.csv')\n",
    "for i in range(len(law_df)):\n",
    "    path = law_df['path'][i]\n",
    "    so_hieu_van_ban = law_df['so_hieu_van_ban'][i]\n",
    "    if so_hieu_van_ban not in engines:\n",
    "        engines[so_hieu_van_ban] = lawquery.Engine(\n",
    "            os.path.join('documents',path, 'tree.json.gz'),\n",
    "        )\n",
    "print(\"Number of engines:\",len(engines))\n",
    "\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "\n",
    "# take all\n",
    "for engine in engines:\n",
    "    results = engines[engine].query(node_type='điều')\n",
    "    for result in results:\n",
    "        documents.append(result.name+'\\n'+result.content)\n",
    "        metadatas.append({'law_id': engine, 'node_type': result.node_type, 'node_id': result.node_id})\n",
    "        ids.append(result.id)\n",
    "\n",
    "qa_df = pd.read_json('./answers_filtered.jsonl', lines=True, orient='records')\n",
    "print(\"Number of documents:\",len(documents))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training triplet using TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdidf_docs= [format_text(doc,True) for doc in documents]\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(tdidf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructor_for_q = 'Represent the legal question for retrieving evidence documents:'\n",
    "instructor_for_r = 'Represent the legal document for retrieval:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4205/4205 [00:23<00:00, 176.80it/s]\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "to_run = len(qa_df)\n",
    "for i in tqdm(range(to_run)):\n",
    "    row = qa_df.iloc[i]\n",
    "    data = {}\n",
    "    query = row['cauhoi']\n",
    "    pos = []\n",
    "    for item in row['new_answers']:\n",
    "        idx = metadatas.index(item)\n",
    "        pos.append(documents[idx])\n",
    "    query = format_text(query,True) \n",
    "    query_vector = tfidf.transform([query])\n",
    "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    related_docs_indices = cosine_similarities.argsort()[:-100-1:-1]\n",
    "    results = []\n",
    "    for j in related_docs_indices:\n",
    "        results.append(documents[j])\n",
    "    neg = []\n",
    "    for j in range(len(results)):\n",
    "        if results[j] not in pos:\n",
    "            neg.append(results[j])\n",
    "        if len(neg) == len(pos):\n",
    "            break\n",
    "\n",
    "    for j in range(len(pos)):\n",
    "        data = {}\n",
    "        data['pos'] = [instructor_for_r,pos[j]]\n",
    "        data['neg'] = [instructor_for_r,neg[j]]\n",
    "        data['query'] = [instructor_for_q,query]\n",
    "        data['task_name'] = 'tracuuluat'\n",
    "        datasets.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6446"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "with gzip.open('../models/cachedir/medi-data.json.gz', 'wt', encoding='utf-8') as fout:\n",
    "    fout.write(json.dumps(datasets, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training triplet using SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "from InstructorEmbedding import INSTRUCTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client(Settings(chroma_api_impl=\"rest\",\n",
    "                                    chroma_server_host=\"localhost\",\n",
    "                                    chroma_server_http_port=\"8000\",\n",
    "                                    chroma_server_ssl_enabled=False\n",
    "                                    ))\n",
    "collection = chroma_client.get_or_create_collection(name=\"law_documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "model = INSTRUCTOR('C:/Users/ngoph/Desktop/luanvan/model')\n",
    "instructor_for_q = 'Represent the legal question for retrieving evidence documents:'\n",
    "instructor_for_r = 'Represent the legal document for retrieval:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Cấp lại sổ BHXH do mất, hỏng, gộp sổ BHXH\\n1.1. Thành phần hồ sơ:\\na) Cấp lại sổ BHXH do mất, hỏng: Tờ khai tham gia, điều chỉnh thông tin BHXH, BHYT (Mẫu TK1-TS). b) Gộp sổ BHXH:\\n- Tờ khai tham gia, điều chỉnh thông tin BHXH, BHYT (Mẫu TK1-TS).\\n- Các sổ BHXH đề nghị gộp (nếu có);\\n1.2. Số lượng hồ sơ: 01 bộ.\\n2. Cấp lại sổ BHXH do thay đổi họ, tên, chữ đệm; ngày, tháng, năm sinh; giới tính, dân tộc; quốc tịch; điều chỉnh nội dung trên sổ BHXH\\n2.1. Thành phần hồ sơ\\na) Người tham gia\\n- Tờ khai tham gia, điều chỉnh thông tin BHXH, BHYT (Mẫu TK1-TS).\\n- Hồ sơ kèm theo (Mục 3,4 Phụ lục 01).\\nb) Đơn vị: Bảng kê thông tin (Mẫu D01-TS).\\n2.2. Số lượng hồ sơ: 01 bộ.\\n3. Ghi xác nhận thời gian đóng BHXH cho người tham gia được cộng nối thời gian nhưng không phải đóng BHXH và điều chỉnh làm nghề hoặc công việc nặng nhọc, độc hại, nguy hiểm hoặc đặc biệt nặng nhọc, độc hại, nguy hiểm trước năm 1995\\n3.1. Thành phần hồ sơ\\na) Tờ khai tham gia, điều chỉnh thông tin BHXH, BHYT (Mẫu TK1-TS).\\nb) Hồ sơ kèm theo (Mục 1, 2 Phụ lục 01).\\n3.2. Số lượng hồ sơ: 01 bộ.\\n4. Cấp lại, đổi thẻ BHYT\\n4.1. Thành phần hồ sơ\\na) Người tham gia\\n- Tờ khai tham gia, điều chỉnh thông tin BHXH, BHYT (Mẫu TK1-TS).\\n-Trường hợp người lao động được hưởng quyền lợi BHYT cao hơn: bổ sung Giấy tờ chứng minh (nếu có) theo Phụ lục 03.\\nb) Đơn vị: Bảng kê thông tin (Mẫu D01-TS).\\n4.2. Số lượng hồ sơ: 01 bộ.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engines['2089/VBHN-BHXH'].query(node_type='điều',node_id='27')[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4205/4205 [02:39<00:00, 26.34it/s]\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "to_run = len(qa_df)\n",
    "for i in tqdm(range(to_run)):\n",
    "    row = qa_df.iloc[i]\n",
    "    data = {}\n",
    "    query = row['cauhoi']\n",
    "    pos = []\n",
    "    for item in row['new_answers']:\n",
    "        idx = metadatas.index(item)\n",
    "        pos.append(documents[idx])\n",
    "    \n",
    "    query_embeddings = model.encode([[instructor_for_q,query]],show_progress_bar=False).tolist()\n",
    "    results = collection.query(query_embeddings=query_embeddings, n_results=100,include=[\"metadatas\"])\n",
    "    results = results['metadatas'][0]\n",
    "    neg = []\n",
    "    for result in results:\n",
    "        doc = engines[result['law_id']].query(node_type=result['node_type'],node_id=result['node_id'])[0]\n",
    "        doc = doc.name+'\\n'+doc.content\n",
    "        if doc not in pos:\n",
    "            neg.append(doc)\n",
    "        if len(neg) == len(pos):\n",
    "            break\n",
    "\n",
    "    for j in range(len(pos)):\n",
    "        data = {}\n",
    "        data['pos'] = [instructor_for_r,pos[j]]\n",
    "        data['neg'] = [instructor_for_r,neg[j]]\n",
    "        data['query'] = [instructor_for_q,query]\n",
    "        data['task_name'] = 'tracuuluat'\n",
    "        datasets.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6446"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('finetune-data-s2.json.gz', 'wt', encoding='utf-8') as fout:\n",
    "    fout.write(json.dumps(datasets, ensure_ascii=False, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

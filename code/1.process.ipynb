{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ngoph\\.conda\\envs\\research\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
            "c:\\Users\\ngoph\\.conda\\envs\\research\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
            "c:\\Users\\ngoph\\.conda\\envs\\research\\lib\\site-packages\\numpy\\.libs\\libopenblas.xwydx2ikjw2nmtwsfyngfuwkqu3lytcz.gfortran-win_amd64.dll\n",
            "c:\\Users\\ngoph\\.conda\\envs\\research\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
            "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import bs4\n",
        "import requests\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from law_query_private import LawQuery,Tree\n",
        "from slugify import slugify\n",
        "import gzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # get luat hien hanh\n",
        "# req = requests.get('https://thuvienphapluat.vn/chinh-sach-phap-luat-moi/vn/thoi-su-phap-luat/tu-van-phap-luat/42248/danh-muc-luat-bo-luat-hien-hanh-tai-viet-nam')\n",
        "# soup = BeautifulSoup(req.text, 'html.parser')\n",
        "# luat_hien_hanh = soup.select('.newcontent a')\n",
        "# urls = []\n",
        "# for i in luat_hien_hanh:\n",
        "#     urls.append(i['href'])\n",
        "# urls = urls[1:]\n",
        "# # save to urls.txt\n",
        "# with open('urls.txt', 'w') as f:\n",
        "#     f.write('\\n'.join(urls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "urls = []\n",
        "with open('urls2.txt', 'r') as f:\n",
        "    urls = f.read().split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_text(url):\n",
        "    # get metadata\n",
        "    metadata_url='https://thuvienphapluat.vn/AjaxLoadData/LoadLuocDo.aspx?LawID={}&IstraiNghiem=True'\n",
        "    id = url.replace('.aspx','').split('-')[-1]\n",
        "    req=requests.get(metadata_url.format(id),headers={'referer': url})\n",
        "    if req.status_code != 200:\n",
        "        raise Exception('Error when get metadata')\n",
        "    soup = BeautifulSoup(req.text, 'html.parser')\n",
        "    metadata = soup.select_one('#viewingDocument')\n",
        "    metadata = {\n",
        "        'ten_van_ban': metadata.select_one('#viewingDocument > div:nth-child(1)').getText(strip=True), \n",
        "        'so_hieu_van_ban': metadata.select_one('#viewingDocument > div:nth-child(2) > div.ds.fl').getText(strip=True),\n",
        "        'loai_van_ban': metadata.select_one('#viewingDocument > div:nth-child(3) > div.ds.fl').getText(strip=True),\n",
        "        'linhvuc': metadata.select_one('#viewingDocument > div:nth-child(4) > div.ds.fl').getText(strip=True),\n",
        "        'noi_ban_hanh': metadata.select_one('#viewingDocument > div:nth-child(5) > div.ds.fl').getText(strip=True),\n",
        "        'nguoi_ky': metadata.select_one('#viewingDocument > div:nth-child(6) > div.ds.fl').getText(strip=True),\n",
        "        'ngay_ban_hanh': metadata.select_one('#viewingDocument > div:nth-child(7) > div.ds.fl').getText(strip=True),\n",
        "        'ngay_hieu_luc': metadata.select_one('#viewingDocument > div:nth-child(8) > div.ds.fl').getText(strip=True),\n",
        "        'ngay_cong_bao': metadata.select_one('#viewingDocument > div:nth-child(9) > div.ds.fl').getText(strip=True),\n",
        "        'so_cong_bao': metadata.select_one('#viewingDocument > div:nth-child(10) > div.ds.fl').getText(strip=True),\n",
        "        'tinh_trang': metadata.select_one('#viewingDocument > div:nth-child(11) > div.ds.fl').getText(strip=True),\n",
        "    }\n",
        "    req=requests.get(url)\n",
        "    soup = BeautifulSoup(req.text, 'html.parser')\n",
        "    html = soup.find('div', {'class':'content1'})\n",
        "    for element in html(text=lambda text: isinstance(text, bs4.Comment)):\n",
        "        element.extract()\n",
        "    for a in html.find_all('a', href=lambda x: x and x[0] == '#'):\n",
        "        a.extract()\n",
        "    for s in html.find_all(['script','style']):\n",
        "        s.extract()\n",
        "    ps = html.find_all(['p','h1','h2','h3','h4','h5','h6'])\n",
        "    content = ''\n",
        "    for p in ps:\n",
        "        text = p.text\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
        "        content += text + '\\n'\n",
        "    # delete - more than 3\n",
        "    content = re.sub(r'-{3,}', '', content)\n",
        "    content = re.sub(r'\\*{3,}', '', content)\n",
        "    # remove line with empty content\n",
        "    content = re.sub(r'^\\s+$', '', content, flags=re.MULTILINE)\n",
        "    # name = soup.find('h1').text.strip()\n",
        "    matches = re.finditer(r'^(chương [\\d\\w]+.*|phần thứ [\\d\\w]+.*)$', content, re.MULTILINE | re.IGNORECASE)\n",
        "    for i,match in enumerate(matches):\n",
        "        end = match.end()\n",
        "        while end < len(content) and content[end] != '\\n':\n",
        "            end += 1\n",
        "        if end < len(content) and content[end] == '\\n':\n",
        "            content = content[:end] + ':' + content[end+1:]\n",
        "    return metadata,content,html.prettify()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Luật Bảo hiểm xã hội 2014\n",
            "Văn bản hợp nhất 2089/VBHN-BHXH năm 2020 hợp nhất Quyết định về Quy trình thu bảo hiểm xã hội, bảo hiểm y tế, bảo hiểm thất nghiệp, bảo hiểm tai nạn lao động, bệnh nghề nghiệp; quản lý sổ bảo hiểm xã hội, thẻ bảo hiểm y tế do Bảo hiểm xã hội Việt Nam ban hành\n",
            "Thông tư 59/2015/TT-BLĐTBXH quy định chi tiết và hướng dẫn thi hành một số điều của Luật bảo hiểm xã hội về bảo hiểm xã hội bắt buộc do Bộ trưởng Bộ Lao động - Thương binh và Xã hội ban hành\n",
            "Nghị định 115/2015/NĐ-CP hướng dẫn Luật bảo hiểm xã hội về bảo hiểm xã hội bắt buộc\n",
            "Nghị định 146/2018/NĐ-CP hướng dẫn Luật bảo hiểm y tế\n",
            "Nghị định 28/2015/NĐ-CP hướng dẫn Luật Việc làm về bảo hiểm thất nghiệp\n",
            "Luật việc làm 2013\n",
            "Bộ luật Lao động 2019\n",
            "Thông tư 56/2017/TT-BYT về hướng dẫn Luật bảo hiểm xã hội và Luật an toàn vệ sinh lao động thuộc lĩnh vực y tế do Bộ trưởng Bộ Y tế ban hành\n",
            "Nghị định 134/2015/NĐ-CP hướng dẫn Luật Bảo hiểm xã hội về bảo hiểm xã hội tự nguyện\n",
            "Quyết định 28/2021/QĐ-TTg quy định về thực hiện chính sách hỗ trợ người lao động và người sử dụng lao động bị ảnh hưởng bởi đại dịch COVID-19 từ Quỹ bảo hiểm thất nghiệp do Thủ tướng Chính phủ ban hành\n",
            "Luật bảo hiểm y tế 2008\n",
            "Luật Bảo hiểm y tế sửa đổi 2014\n",
            "Quyết định 166/QĐ-BHXH năm 2019 về Quy trình giải quyết hưởng chế độ bảo hiểm xã hội, chi trả chế độ bảo hiểm xã hội, bảo hiểm thất nghiệp do Bảo hiểm xã hội Việt Nam ban hành\n",
            "Nghị định 61/2020/NĐ-CP về sửa đổi Nghị định 28/2015/NĐ-CP hướng dẫn Luật Việc làm về bảo hiểm thất nghiệp\n"
          ]
        }
      ],
      "source": [
        "items = []\n",
        "for url in urls:\n",
        "    try:\n",
        "        metadata,text,html = get_text(url)\n",
        "        print(metadata['ten_van_ban'])\n",
        "        name_slug = slugify(metadata['ten_van_ban'])[:50]\n",
        "        items.append({'metadata':metadata,'path':name_slug})\n",
        "        folder_path = os.path.join('documents', name_slug)\n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "        with open(os.path.join(folder_path, 'content.txt'), 'w', encoding='utf-8') as f:\n",
        "            f.write(text)\n",
        "        with open(os.path.join(folder_path, 'content.html'), 'w', encoding='utf-8') as f:\n",
        "            f.write(html)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(url)\n",
        "        continue\n",
        "df = pd.DataFrame(items)\n",
        "# expose metadata\n",
        "df = pd.concat([df.drop(['metadata'], axis=1), df['metadata'].apply(pd.Series)], axis=1)\n",
        "df.to_csv('documents/data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_content_for_tree(text):\n",
        "    re_footer = r'^(Bộ luật này đã được Quốc hội .*|Luật này được Quốc hội.*|Bộ luật này được Quốc hội .*|Luật này đã được Quốc hội .*)'\n",
        "    matches = re.split(re_footer, text, flags=re.MULTILINE)\n",
        "    text = matches[0]\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "luat-bao-hiem-xa-hoi-2014\n"
          ]
        },
        {
          "ename": "JSONDecodeError",
          "evalue": "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m         f\u001b[39m.\u001b[39mwrite(text)\n\u001b[0;32m     12\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_path, \u001b[39m'\u001b[39m\u001b[39mcontent_tree.txt\u001b[39m\u001b[39m'\u001b[39m), encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mread()\n\u001b[1;32m---> 13\u001b[0m tree \u001b[39m=\u001b[39m Tree(metadata\u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(row\u001b[39m.\u001b[39;49mmetadata),content\u001b[39m=\u001b[39mtext,raw_text\u001b[39m=\u001b[39mraw_text)\n\u001b[0;32m     14\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_path, \u001b[39m'\u001b[39m\u001b[39mdebug_tree.txt\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     15\u001b[0m     f\u001b[39m.\u001b[39mwrite(\u001b[39mstr\u001b[39m(tree))\n",
            "File \u001b[1;32mc:\\Users\\ngoph\\.conda\\envs\\research\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
            "File \u001b[1;32mc:\\Users\\ngoph\\.conda\\envs\\research\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
            "File \u001b[1;32mc:\\Users\\ngoph\\.conda\\envs\\research\\lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[39ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[39mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)"
          ]
        }
      ],
      "source": [
        "CREATE_CONTENT_TREE=True\n",
        "df = pd.read_csv('documents/data.csv')\n",
        "for row in df.itertuples():\n",
        "    print(row.path)\n",
        "    folder_path = os.path.join('documents', row.path)\n",
        "    text = open(os.path.join(folder_path, 'content.txt'), encoding='utf-8').read()\n",
        "    raw_text = text\n",
        "    text = make_content_for_tree(text)\n",
        "    if CREATE_CONTENT_TREE:\n",
        "        with open(os.path.join(folder_path, 'content_tree.txt'), 'w', encoding='utf-8') as f:\n",
        "            f.write(text)\n",
        "    text = open(os.path.join(folder_path, 'content_tree.txt'), encoding='utf-8').read()\n",
        "    tree = Tree(metadata= json.loads(row.metadata),content=text,raw_text=raw_text)\n",
        "    with open(os.path.join(folder_path, 'debug_tree.txt'), 'w', encoding='utf-8') as f:\n",
        "        f.write(str(tree))\n",
        "    with open(os.path.join(folder_path, 'tree.pkl'), 'wb') as f:\n",
        "        pickle.dump(tree, f)\n",
        "    with open(os.path.join(folder_path, 'tree.json'), 'w', encoding='utf-8') as f:\n",
        "        f.write(json.dumps(tree.export(), indent=4, ensure_ascii=False))\n",
        "    with open(os.path.join(folder_path, 'tree.json.gz'), 'wb') as f:\n",
        "        f.write(gzip.compress(json.dumps(tree.export(), ensure_ascii=False).encode('utf-8')))\n",
        "    # with open('documents/{}.json.gz'.format(row.path), 'wb') as f:\n",
        "        # f.write(gzip.compress(json.dumps(tree.export(), ensure_ascii=False).encode('utf-8')))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "research",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

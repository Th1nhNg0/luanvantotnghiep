{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import bs4\n",
        "import requests\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from law_query_private import LawQuery,Tree\n",
        "from slugify import slugify\n",
        "import gzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # get luat hien hanh\n",
        "# req = requests.get('https://thuvienphapluat.vn/chinh-sach-phap-luat-moi/vn/thoi-su-phap-luat/tu-van-phap-luat/42248/danh-muc-luat-bo-luat-hien-hanh-tai-viet-nam')\n",
        "# soup = BeautifulSoup(req.text, 'html.parser')\n",
        "# luat_hien_hanh = soup.select('.newcontent a')\n",
        "# urls = []\n",
        "# for i in luat_hien_hanh:\n",
        "#     urls.append(i['href'])\n",
        "# urls = urls[1:]\n",
        "# # save to urls.txt\n",
        "# with open('urls.txt', 'w') as f:\n",
        "#     f.write('\\n'.join(urls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "urls = []\n",
        "with open('urls2.txt', 'r') as f:\n",
        "    urls = f.read().split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_text(url):\n",
        "    # get metadata\n",
        "    metadata_url='https://thuvienphapluat.vn/AjaxLoadData/LoadLuocDo.aspx?LawID={}&IstraiNghiem=True'\n",
        "    id = url.replace('.aspx','').split('-')[-1]\n",
        "    req=requests.get(metadata_url.format(id),headers={'referer': url})\n",
        "    if req.status_code != 200:\n",
        "        raise Exception('Error when get metadata')\n",
        "    soup = BeautifulSoup(req.text, 'html.parser')\n",
        "    metadata = soup.select_one('#viewingDocument')\n",
        "    metadata = {\n",
        "        'ten_van_ban': metadata.select_one('#viewingDocument > div:nth-child(1)').getText(strip=True), \n",
        "        'so_hieu_van_ban': metadata.select_one('#viewingDocument > div:nth-child(2) > div.ds.fl').getText(strip=True),\n",
        "        'loai_van_ban': metadata.select_one('#viewingDocument > div:nth-child(3) > div.ds.fl').getText(strip=True),\n",
        "        'linhvuc': metadata.select_one('#viewingDocument > div:nth-child(4) > div.ds.fl').getText(strip=True),\n",
        "        'noi_ban_hanh': metadata.select_one('#viewingDocument > div:nth-child(5) > div.ds.fl').getText(strip=True),\n",
        "        'nguoi_ky': metadata.select_one('#viewingDocument > div:nth-child(6) > div.ds.fl').getText(strip=True),\n",
        "        'ngay_ban_hanh': metadata.select_one('#viewingDocument > div:nth-child(7) > div.ds.fl').getText(strip=True),\n",
        "        'ngay_hieu_luc': metadata.select_one('#viewingDocument > div:nth-child(8) > div.ds.fl').getText(strip=True),\n",
        "        'ngay_cong_bao': metadata.select_one('#viewingDocument > div:nth-child(9) > div.ds.fl').getText(strip=True),\n",
        "        'so_cong_bao': metadata.select_one('#viewingDocument > div:nth-child(10) > div.ds.fl').getText(strip=True),\n",
        "        'tinh_trang': metadata.select_one('#viewingDocument > div:nth-child(11) > div.ds.fl').getText(strip=True),\n",
        "    }\n",
        "    req=requests.get(url)\n",
        "    soup = BeautifulSoup(req.text, 'html.parser')\n",
        "    html = soup.find('div', {'class':'content1'})\n",
        "    for element in html(text=lambda text: isinstance(text, bs4.Comment)):\n",
        "        element.extract()\n",
        "    for a in html.find_all('a', href=lambda x: x and x[0] == '#'):\n",
        "        a.extract()\n",
        "    for s in html.find_all(['script','style']):\n",
        "        s.extract()\n",
        "    ps = html.find_all(['p','h1','h2','h3','h4','h5','h6'])\n",
        "    content = ''\n",
        "    for p in ps:\n",
        "        text = p.text\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
        "        content += text + '\\n'\n",
        "    # delete - more than 3\n",
        "    content = re.sub(r'-{3,}', '', content)\n",
        "    content = re.sub(r'\\*{3,}', '', content)\n",
        "    # remove line with empty content\n",
        "    content = re.sub(r'^\\s+$', '', content, flags=re.MULTILINE)\n",
        "    # name = soup.find('h1').text.strip()\n",
        "    matches = re.finditer(r'^(chương [\\d\\w]+.*|phần thứ [\\d\\w]+.*)$', content, re.MULTILINE | re.IGNORECASE)\n",
        "    for i,match in enumerate(matches):\n",
        "        end = match.end()\n",
        "        while end < len(content) and content[end] != '\\n':\n",
        "            end += 1\n",
        "        if end < len(content) and content[end] == '\\n':\n",
        "            content = content[:end] + ':' + content[end+1:]\n",
        "    return metadata,content,html.prettify()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Luật Bảo hiểm xã hội 2014\n",
            "Văn bản hợp nhất 2089/VBHN-BHXH năm 2020 hợp nhất Quyết định về Quy trình thu bảo hiểm xã hội, bảo hiểm y tế, bảo hiểm thất nghiệp, bảo hiểm tai nạn lao động, bệnh nghề nghiệp; quản lý sổ bảo hiểm xã hội, thẻ bảo hiểm y tế do Bảo hiểm xã hội Việt Nam ban hành\n",
            "Thông tư 59/2015/TT-BLĐTBXH quy định chi tiết và hướng dẫn thi hành một số điều của Luật bảo hiểm xã hội về bảo hiểm xã hội bắt buộc do Bộ trưởng Bộ Lao động - Thương binh và Xã hội ban hành\n",
            "Nghị định 115/2015/NĐ-CP hướng dẫn Luật bảo hiểm xã hội về bảo hiểm xã hội bắt buộc\n",
            "Nghị định 146/2018/NĐ-CP hướng dẫn Luật bảo hiểm y tế\n",
            "Nghị định 28/2015/NĐ-CP hướng dẫn Luật Việc làm về bảo hiểm thất nghiệp\n",
            "Luật việc làm 2013\n",
            "Bộ luật Lao động 2019\n",
            "Thông tư 56/2017/TT-BYT về hướng dẫn Luật bảo hiểm xã hội và Luật an toàn vệ sinh lao động thuộc lĩnh vực y tế do Bộ trưởng Bộ Y tế ban hành\n",
            "Nghị định 134/2015/NĐ-CP hướng dẫn Luật Bảo hiểm xã hội về bảo hiểm xã hội tự nguyện\n",
            "Quyết định 28/2021/QĐ-TTg quy định về thực hiện chính sách hỗ trợ người lao động và người sử dụng lao động bị ảnh hưởng bởi đại dịch COVID-19 từ Quỹ bảo hiểm thất nghiệp do Thủ tướng Chính phủ ban hành\n",
            "Luật bảo hiểm y tế 2008\n",
            "Luật Bảo hiểm y tế sửa đổi 2014\n",
            "Quyết định 166/QĐ-BHXH năm 2019 về Quy trình giải quyết hưởng chế độ bảo hiểm xã hội, chi trả chế độ bảo hiểm xã hội, bảo hiểm thất nghiệp do Bảo hiểm xã hội Việt Nam ban hành\n",
            "Nghị định 61/2020/NĐ-CP về sửa đổi Nghị định 28/2015/NĐ-CP hướng dẫn Luật Việc làm về bảo hiểm thất nghiệp\n"
          ]
        }
      ],
      "source": [
        "items = []\n",
        "for url in urls:\n",
        "    try:\n",
        "        metadata,text,html = get_text(url)\n",
        "        print(metadata['ten_van_ban'])\n",
        "        name_slug = slugify(metadata['ten_van_ban'])[:50]\n",
        "        items.append({'metadata':metadata,'path':name_slug})\n",
        "        folder_path = os.path.join('documents', name_slug)\n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "        with open(os.path.join(folder_path, 'content.txt'), 'w', encoding='utf-8') as f:\n",
        "            f.write(text)\n",
        "        with open(os.path.join(folder_path, 'content.html'), 'w', encoding='utf-8') as f:\n",
        "            f.write(html)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(url)\n",
        "        continue\n",
        "df = pd.DataFrame(items)\n",
        "# expose metadata\n",
        "df = pd.concat([df, df['metadata'].apply(pd.Series)], axis=1)\n",
        "df['metadata'] = df['metadata'].apply(lambda x: json.dumps(x, ensure_ascii=False))\n",
        "df.to_csv('documents/data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_content_for_tree(text):\n",
        "    re_footer = r'^(Bộ luật này đã được Quốc hội .*|Luật này được Quốc hội.*|Bộ luật này được Quốc hội .*|Luật này đã được Quốc hội .*)'\n",
        "    matches = re.split(re_footer, text, flags=re.MULTILINE)\n",
        "    text = matches[0]\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "luat-bao-hiem-xa-hoi-2014\n",
            "van-ban-hop-nhat-2089-vbhn-bhxh-nam-2020-hop-nhat-\n",
            "thong-tu-59-2015-tt-bldtbxh-quy-dinh-chi-tiet-va-h\n",
            "nghi-dinh-115-2015-nd-cp-huong-dan-luat-bao-hiem-x\n",
            "nghi-dinh-146-2018-nd-cp-huong-dan-luat-bao-hiem-y\n",
            "nghi-dinh-28-2015-nd-cp-huong-dan-luat-viec-lam-ve\n",
            "luat-viec-lam-2013\n",
            "bo-luat-lao-dong-2019\n",
            "thong-tu-56-2017-tt-byt-ve-huong-dan-luat-bao-hiem\n",
            "nghi-dinh-134-2015-nd-cp-huong-dan-luat-bao-hiem-x\n",
            "quyet-dinh-28-2021-qd-ttg-quy-dinh-ve-thuc-hien-ch\n",
            "luat-bao-hiem-y-te-2008\n",
            "luat-bao-hiem-y-te-sua-doi-2014\n",
            "quyet-dinh-166-qd-bhxh-nam-2019-ve-quy-trinh-giai-\n",
            "nghi-dinh-61-2020-nd-cp-ve-sua-doi-nghi-dinh-28-20\n"
          ]
        }
      ],
      "source": [
        "CREATE_CONTENT_TREE=True\n",
        "df = pd.read_csv('documents/data.csv')\n",
        "for row in df.itertuples():\n",
        "    print(row.path)\n",
        "    folder_path = os.path.join('documents', row.path)\n",
        "    text = open(os.path.join(folder_path, 'content.txt'), encoding='utf-8').read()\n",
        "    raw_text = text\n",
        "    text = make_content_for_tree(text)\n",
        "    if CREATE_CONTENT_TREE:\n",
        "        with open(os.path.join(folder_path, 'content_tree.txt'), 'w', encoding='utf-8') as f:\n",
        "            f.write(text)\n",
        "    text = open(os.path.join(folder_path, 'content_tree.txt'), encoding='utf-8').read()\n",
        "    tree = Tree(metadata= json.loads(row.metadata),content=text,raw_text=raw_text)\n",
        "    with open(os.path.join(folder_path, 'debug_tree.txt'), 'w', encoding='utf-8') as f:\n",
        "        f.write(str(tree))\n",
        "    with open(os.path.join(folder_path, 'tree.pkl'), 'wb') as f:\n",
        "        pickle.dump(tree, f)\n",
        "    with open(os.path.join(folder_path, 'tree.json'), 'w', encoding='utf-8') as f:\n",
        "        f.write(json.dumps(tree.export(), indent=4, ensure_ascii=False))\n",
        "    with open(os.path.join(folder_path, 'tree.json.gz'), 'wb') as f:\n",
        "        f.write(gzip.compress(json.dumps(tree.export(), ensure_ascii=False).encode('utf-8')))\n",
        "    # with open('documents/{}.json.gz'.format(row.path), 'wb') as f:\n",
        "        # f.write(gzip.compress(json.dumps(tree.export(), ensure_ascii=False).encode('utf-8')))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "research",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
